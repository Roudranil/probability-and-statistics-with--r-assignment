---
title: "Assignment 2 code"
author: "Roudranil Das, Shreyan Chakraborty, Saikat Bera"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align = 'center')
knitr::opts_chunk$set(fig = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

# Problem 1

Suppose $X$ denote the number of goals scored by home team in premier league. We can assume $X$ is a random variable. Then we have to build the probability distribution to model the probability of number of goals. Since $X$ takes value in $\mathbb{N}=\{0,1,2,\cdots\}$, we can consider the geometric progression sequence as possible candidate model, i.e.,
$$
S=\{a,ar,ar^2,ar^{3},\cdots\}.
$$
But we have to be careful and put proper conditions in place and modify $S$ in such a way so that it becomes proper probability distributions. 


## Part 1 
Figure out the necesary conditions and define the probability distribution model using $S$.       

We are given a random variable $X$ with a supposed prob. distribution
$\{a, ar, ar^2, ...\}$. Hence, we can write: $$\mathbb{P}(X=i) = ar^i$$

Now, because $\sum_{i} \mathbb{P}(X=i) = 1$ for a probability
distribution, we must have \begin{align*} \sum_{i} \mathbb{P}(X=i) &= 1
\\ \implies \sum_{i} ar^i &= 1 \\ \implies \frac{a}{1-r} &= 1 \\
\implies a &= 1-r \end{align*}

We have ***assumed*** that $|r| < 1$, but because probabilities are
non-negative, it's sufficient to have: $0 \le r < 1$.

Hence, we finally have $$\mathbb{P}(X=i) = (1-r)r^i, ~~~ 0\le r < 1$$

## Parts 2 and 3   
2. Check if mean and variance exists for the probability model.  
3. Can you find the analytically expression of mean and variance.   

Once again, we ***assume*** $|r| < 1$, but because probabilities are
non-negative, it's sufficient to have: $0 \le r < 1$.

We now have: \begin{align*} \mathbb{E}(X) &= \sum_{i} i\mathbb{P}(X=i)
\\ &= \sum_{i} i(1-r)r^i \\ \implies \frac{\mathbb{E}(X)}{1-r} &=
\sum_{i} ir^i \\ \implies \frac{\mathbb{E}(X)}{1-r} &= \frac{r}{(1-r)^2}
\\ \implies \mathbb{E}(X) &= \frac{r}{1-r} \end{align*}

Observe that \begin{align*} \mathbb{E}(X^2) &= \sum_{i} i^2
\mathbb{P}(X=i) \\ &= \sum_{i} i^2(1-r)r^i \end{align*}

Calculating the sum, we obtain: $$\mathbb{E}(X^2) =
\frac{r(r+1)}{(1-r)^2}$$

To calculate the variance, now: \begin{align*} \text{Var}(X) &=
\mathbb{E}(X^2) - [\mathbb{E}(X)]^2 \\ &= \frac{r(r+1)}{(1-r)^2} -
\frac{r^2}{(1-r)^2} \\ &= \frac{r}{(1-r)^2} \end{align*}

## Part 4
From historical data we found the following summary statistics
\begin{table}[ht]
\centering
     \begin{tabular}{c|c|c|c}\hline
     mean &  median & variance & total number of matches\\ \hline
     1.5 & 1 & 2.25 & 380\\ \hline
     \end{tabular}
\end{table}
Using the summary statistics and your newly defined probability distribution model find the following:
 a. What is the probability that home team will score at least one goal?
 b. What is the probability that home team will score at least one goal but less than four goal?    

We are given that the mean is 1.5 and the variance is 2.25. This amounts
to solving the following set of equations:
\begin{align} \frac{r}{1-r} &= 1.5 \\ \frac{r}{(1-r)^2} &= 2.25
\end{align}

But, one can easily verify that this system of equations is
inconsistent. So, we cannot solve for the value of $r$ using this
method. To resolve this issue, we are going to find the ***maximum
likelihood estimate*** of $r$.

Denote by $\mathbf{x}$ the observed values in a random sample $x_1, x_2,
\cdots, x_n$. The likelihood function for the geometric distribution can
be expressed as: $$L(r|\mathbf{x}) = \prod_{i=1}^{n} (1-r)r^{x_i} =
(1-r)^n\,r^{\sum_{i=1}^{n} x_i}$$

Taking the natural logarithm of the likelihood function gives:
$$ \ln L(r|\mathbf{x}) = \ln \big[(1-r)^n\,r^{\sum_{i=1}^{n} x_i}\big] =
n \ln(1-r) + \ln(r) \sum x_i \tag{a}$$

Let's take the first-order partial derivative of $\ln L(r|\mathbf{x})$
with respect to $r$ and set the answer equal to zero:
$$\frac{\partial \ln L(r|\mathbf{x})}{\partial r} = -\frac{n}{1-r} +
\frac{\sum x_i}{r} \stackrel{set}{=} 0$$

The solution is given by $\hat{r} = \frac{\sum x_i}{\sum x_i + n}$. It's
easy to check that the second-order partial derivative of the
log-likelihood function is negative at $r = \hat{r}$.

For our problem, let's find this value from the summary of data we're
given:
$$\hat{r} = \frac{380 * 1.5}{380 * 1.5 + 380} = \frac{3}{5}$$

Consequently,

a.  $\mathbb{P}(X \geq 1) = 1 - \mathbb{P}(X = 0) = 1 - (1 - r) = 0.6$;
b.  $\mathbb{P}(1 \leq X < 4) = \mathbb{P}(X = 1) + \mathbb{P}(X = 2) +
\mathbb{P}(X = 3) = (1-r)\big[r + r^2 + r^3] = 0.4704$

## Part 5 
Suppose on another thought you want to model it with off-the shelf Poisson probability models. Under the assumption that underlying distribution is Poisson probability find the above probabilities, i.e.,
 a. What is the probability that home team will score at least one goal?
 b. What is the probability that home team will score at least one goal but less than four goal?    

For a Poisson distribution, the mean and variance is equal to $\lambda$.
Furthermore, we know that the ***maximum likelihood estimate*** for
$\lambda$ is the sample mean. So, we will take $\lambda = 1.5$.

a.  $\mathbb{P}(X \geq 1) = 1 - \mathbb{P}(X = 0) = 1 - e^{-\lambda} =
0.78$;
b.  $\mathbb{P}(1 \leq X < 4) = \mathbb{P}(X = 1) + \mathbb{P}(X = 2) +
\mathbb{P}(X = 3) = e^{-\lambda} \big[\lambda + \frac{\lambda^2}{2} +
\frac{\lambda^3}{6}\big] = 0.71$

## Part 6 
Which probability model you would prefer over another?     

When we first see the observed statistics such as mean and variance, we see that sample variance is more than that of sample mean.

Generally speaking,geometric distribution has its variance more than that of mean.

But we see that,on further inspection the sample mean and sample variance is inconsistent.Therefore the given geometric distribution is not that of great fit.

Looking at the observed probabilities for both (a) and (b) in Parts 4
and 5, we are inclined towards the Poisson model as likely to better fit
the data.

## Part 7  
Write down the likelihood functions of your newly defined probability models and Poisson models. Clearly mention all the assumptions that you are making.     

For the Poisson distribution: $$L(\lambda|\mathbf{x}) = \prod_{i=1}^{n}
\frac{e^{-\lambda}\lambda^{x_i}}{x_i!} = e^{-n\lambda}
\prod_{i=1}^{n}\frac{\lambda^{x_i}}{x_i!}$$

For the geometric distribution: $$L(r|\mathbf{x}) = \prod_{i=1}^{n}
(1-r)r^{x_i} = (1-r)^n\,r^{\sum_{i=1}^{n} x_i}$$

# Problem 2

```{r}
library(tidyverse)
```

## Part 1 
```{r warning=FALSE}
mle <- function(log_alpha, data, sigma) {
    l = sum(log(dgamma(data, shape = exp(log_alpha), scale = sigma)))
    # print(paste("l is ", l))
    return(-l)
}

MyMLE <- function(data, sigma) {
    log_alpha_initial <- log(mean(data)^2/var(data))
    # print(paste("log alpha initial is ", log_alpha_initial))
    estimator <- optim(log_alpha_initial,
                 mle,
                 data = data,
                 sigma = sigma)
    log_alpha_hat <- estimator$par
    return(log_alpha_hat)
}
```


```{r warning=FALSE}
get_estimates <- function(n, alpha, sigma) {
    estimates <- c()
    for (i in 1:1000) {
        samples <- rgamma(n, shape = alpha, scale = sigma)
        # print(paste("some of the samples are ", samples[1:5]))
        estimates <- append(estimates, MyMLE(data = samples, sigma = sigma))
    }
    return(estimates)
}
```

## Part 2
```{r}
n = 20
alpha = 1.5
sigma = 2.2

estimated_mle <- tibble(get_estimates(n = n, alpha = alpha, sigma = sigma))
colnames(estimated_mle) <- c("estimate")

perc_2.5 <- quantile(estimated_mle$estimate, probs = 0.025, names = FALSE)
perc_97.5 <- quantile(estimated_mle$estimate, probs = 0.975, names = FALSE)

estimated_mle %>% 
    ggplot(aes(estimate)) +
    geom_histogram(color = "white", fill = "#5D5D5D") +
    geom_vline(xintercept = log(alpha), 
               size = 2,
               linetype = "dashed") +
    annotate("text", label = "Actual log(alpha)", x = 0.5, y = 95, color = "black") +
    geom_vline(xintercept = perc_2.5, 
               color = "#00B9FF", size = 1.5, linetype = "dashed") +
    annotate("text", label = "2.5 percentile", x = perc_2.5 + 0.1, y = 95, color = "#00B9FF") +
    geom_vline(xintercept = perc_97.5, 
               color = "#E08304", size = 1.5, linetype = "dashed") +
    annotate("text", label = "97.5 percentile", x = perc_97.5 + 0.1, y = 95, color = "#E08304") +
    labs(title = paste("n = ", n, ", alpha = ", alpha, ", sigma = ", sigma),
         x = "Estimated MLE",
         y = "Count")

diff_20 <- perc_97.5 - perc_2.5
```

## Part 3
```{r}
n = 40
alpha = 1.5
sigma = 2.2

estimated_mle <- tibble(get_estimates(n = n, alpha = alpha, sigma = sigma))
colnames(estimated_mle) <- c("estimate")

perc_2.5 <- quantile(estimated_mle$estimate, probs = 0.025, names = FALSE)
perc_97.5 <- quantile(estimated_mle$estimate, probs = 0.975, names = FALSE)

estimated_mle %>% 
    ggplot(aes(estimate)) +
    geom_histogram(color = "white", fill = "#5D5D5D") +
    geom_vline(xintercept = log(alpha), 
               size = 2,
               linetype = "dashed") +
    annotate("text", label = "Actual log(alpha)", x = log(alpha) + 0.1, y = 95, color = "black") +
    geom_vline(xintercept = perc_2.5, 
               color = "#00B9FF", size = 1.5, linetype = "dashed") +
    annotate("text", label = "2.5 percentile", x = perc_2.5 + 0.1, y = 95, color = "#00B9FF") +
    geom_vline(xintercept = perc_97.5, 
               color = "#E08304", size = 1.5, linetype = "dashed") +
    annotate("text", label = "97.5 percentile", x = perc_97.5 + 0.1, y = 95, color = "#E08304") +
    labs(title = paste("n = ", n, ", alpha = ", alpha, ", sigma = ", sigma),
         x = "Estimated MLE",
         y = "Count")

diff_40 <- perc_97.5 - perc_2.5
```

## Part 4 
```{r}
n = 100
alpha = 1.5
sigma = 2.2

estimated_mle <- tibble(get_estimates(n = n, alpha = alpha, sigma = sigma))
colnames(estimated_mle) <- c("estimate")

perc_2.5 <- quantile(estimated_mle$estimate, probs = 0.025, names = FALSE)
perc_97.5 <- quantile(estimated_mle$estimate, probs = 0.975, names = FALSE)

estimated_mle %>% 
    ggplot(aes(estimate)) +
    geom_histogram(color = "white", fill = "#5D5D5D") +
    geom_vline(xintercept = log(alpha), 
               size = 2,
               linetype = "dashed") +
    annotate("text", label = "Actual log(alpha)", x = log(alpha) + 0.05, y = 95, color = "black") +
    geom_vline(xintercept = perc_2.5, 
               color = "#00B9FF", size = 1.5, linetype = "dashed") +
    annotate("text", label = "2.5 percentile", x = perc_2.5 + 0.05, y = 95, color = "#00B9FF") +
    geom_vline(xintercept = perc_97.5, 
               color = "#E08304", size = 1.5, linetype = "dashed") +
    annotate("text", label = "97.5 percentile", x = perc_97.5 + 0.05, y = 95, color = "#E08304") +
    labs(title = paste("n = ", n, ", alpha = ", alpha, ", sigma = ", sigma),
         x = "Estimated MLE",
         y = "Count")

diff_100 <- perc_97.5 - perc_2.5
```

```{r}
diff_20
diff_40
diff_100
```

We can see that the gap between the percentile points is decreasing as the sample size increases.

# Problem 3

```{r}
library(tidyverse)
library(scales)
# library(AICcmodavg)
```

```{r}
data_q3 <- faithful %>% 
    as_tibble()

x <- sort(data_q3$waiting)

# hist(x, xlab = 'waiting', probability = T, col='pink', main='')

```

comparing 3 models

```{r}
# model 1

p <- length(x[x<65])/length(x)

as <- mean(x[x<65])
ass <- var(x[x<65])
s <- ass/as
a <- as/s

mu <- mean(x[x>=65])
sigma <- sd(x[x>=65])

theta_inital <- c(p, a, s, mu, sigma)

neg_log_likelihood <- function(theta, data){
    n = length(data)
    
    p = theta[1]
    a = theta[2]
    s = theta[3]
    mu = theta[4]
    sigma = theta[5]
    
    l = 0
    for (i in 1:n) {
        l = l + log(p*dgamma(data[i], shape = a, scale = s) + (1-p)*dnorm(data[i], mean = mu, sd = sigma))
    }
    return(-l)
}

fit = optim(theta_inital,
      neg_log_likelihood,
      data = x,
      control = list(maxit = 1500),
      lower = c(0, 0, 0, -Inf, 0),
      upper = c(1, Inf, Inf, Inf, Inf),
      method="L-BFGS-B")

theta_1 = fit$par
theta_1

p = theta_1[1]
a = theta_1[2]
s = theta_1[3]
mu = theta_1[4]
sigma = theta_1[5]

model_1 = p*dgamma(x, shape = a, scale = s) + (1-p)*dnorm(x, mean = mu, sd = sigma)

aic_1 <- 2*5 + 2*neg_log_likelihood(theta_1, x)

# hist(x, xlab = 'waiting', probability = T, col='pink', main='')
# lines(x, model_1)
```

```{r}
# model 2

p <- length(x[x<65])/length(x)

as_1 <- mean(x[x<65])
ass_1 <- var(x[x<65])
s_1 <- ass_1/as_1
a_1 <- as_1/s_1

as_2 <- mean(x[x>=65])
ass_2 <- var(x[x>=65])
s_2 <- ass_2/as_2
a_2 <- as_2/s_2

theta_inital <- c(p, a_1, s_1, a_2, s_2)

neg_log_likelihood <- function(theta, data){
    n <- length(data)
    
    p <- theta[1]
    a_1 <- theta[2]
    s_1 <- theta[3]
    a_2 <- theta[4]
    s_2 <- theta[5]
    
    l <- 0
    for (i in 1:n) {
        l = l + log(p*dgamma(data[i], shape = a_1, scale = s_1) + (1-p)*dgamma(data[i], shape = a_2, scale = s_2))
    }
    return(-l)
}

fit = optim(theta_inital,
      neg_log_likelihood,
      data = x,
      control = list(maxit = 1500),
      lower = c(0, 0, 0, 0, 0),
      upper = c(1, Inf, Inf, Inf, Inf),
      method="L-BFGS-B")

theta_2 <- fit$par
theta_2

p <- theta_2[1]
a_1 <- theta_2[2]
s_1 <- theta_2[3]
a_2 <- theta_2[4]
s_2 <- theta_2[5]

model_2 <- p*dgamma(x, shape = a_1, scale = s_1) + (1-p)*dgamma(x, shape = a_2, scale = s_2)

aic_2 <- 2*5 + 2*neg_log_likelihood(theta_2, x)

# hist(x, xlab = 'waiting', probability = T, col='pink', main='')
# lines(x, model_2)
```

```{r}
# model 3

p <- length(x[x<65])/length(x)

m_1 <- mean(x[x<65])
v_1 <- var(x[x<65])
sigma2_1 <- log((v_1/m_1^2) + 1)
mu_1 <- log(m_1) - sigma2_1/2

m_2 <- mean(x[x>=65])
v_2 <- var(x[x>=65])
sigma2_2 <- log((v_2/m_2^2) + 1)
mu_2 <- log(m_2) - sigma2_2/2

theta_inital <- c(p, mu_1, sqrt(sigma2_1), mu_2, sqrt(sigma2_2))

neg_log_likelihood <- function(theta, data) {
    n <- length(data)
    
    p <- theta[1]
    mu_1 <- theta[2]
    sigma_1 <- theta[3]
    mu_2 <- theta[4]
    sigma_2 <- theta[5]
    
    l <- 0
    for (i in 1:n) {
        l = l + log(p*dlnorm(data[i], meanlog = mu_1, sdlog = sigma_1) + (1-p)*dlnorm(data[i], meanlog = mu_2, sdlog = sigma_2))
    }
    
    return(-l)
}

fit = optim(theta_inital,
      neg_log_likelihood,
      data = x,
      control = list(maxit = 1500),
      lower = c(0, -Inf, 0, -Inf, 0),
      upper = c(1, Inf, Inf, Inf, Inf),
      method="L-BFGS-B")

theta_3 <- fit$par
theta_3

p <- theta_3[1]
mu_1 <- theta_3[2]
sigma_1 <- theta_3[3]
mu_2 <- theta_3[4]
sigma_2 <- theta_3[5]

model_3 <- p*dlnorm(x, meanlog = mu_1, sdlog = sigma_1) + (1-p)*dlnorm(x, meanlog = mu_2, sdlog = sigma_2)

aic_3 <- 2*5 + 2*neg_log_likelihood(theta_3, x)

# hist(x, xlab = 'waiting', probability = T, col='pink', main='')
# lines(x, model_3)
```

```{r}
hist(x, xlab = 'waiting', probability = T, col='pink', main='')
lines(x, model_1, col = "red")
lines(x, model_2, col = "blue")
lines(x, model_3, col = "green")
```

```{r}
results <- data.frame(
    models = c("Gamma + Normal", "Gamma + Gamma", "Lognormal + Lognormal"),
    AIC = c(aic_1, aic_2, aic_3)
)
results
```

Based on the AIC value of all the models, we choose the third model as it has the lowest AIC value.

The required probability $\mathbb{P}[60<\texttt{waiting}<70]$ is:
```{r}
p <- theta_3[1]
mu_1 <- theta_3[2]
sigma_1 <- theta_3[3]
mu_2 <- theta_3[4]
sigma_2 <- theta_3[5]

reqd_prob <- (p*plnorm(70, meanlog = mu_1, sdlog = sigma_1) + (1-p)*plnorm(70, meanlog = mu_2, sdlog = sigma_2)) - (p*plnorm(60, meanlog = mu_1, sdlog = sigma_1) + (1-p)*plnorm(60, meanlog = mu_2, sdlog = sigma_2))
```

Hence $\mathbb{P}[60<\texttt{waiting}<70]$ =  `r reqd_prob`

# Problem 4

## Part A

### (i)

```{r results = FALSE}
library(tidyverse)
```

```{r}
library(MASS)
plot(Insurance$Holders,Insurance$Claims
     ,xlab = 'Holders',ylab='Claims',pch=20)
grid()
```

```{r results = FALSE, warning=FALSE}
NegLogLikeA = function(theta,data){
  sigma = exp(theta[3])
  n = nrow(data)
  l=0
  for(i in 1:n){
    mu = theta[1]+theta[2]*data$Holders[i]
    l = l + log(dnorm(data$Claims[i],mean = mu, sd=sigma))
  }
  return(-l)
}


theta_initial = c(0.01,0.1,log(10))
NegLogLikeA(theta_initial,Insurance)

fit = optim(theta_initial,
            NegLogLikeA,
            data = Insurance)
```


```{r}
ggplot(data=Insurance)+
  geom_line(aes(Holders, fit$par[1]+fit$par[2]*Holders))+
  geom_point(aes(Holders,Claims))
theta_hat = fit$par
theta_hat
```

### (ii)

```{r results = FALSE, warning=FALSE}
BIC = 2*NegLogLikeA(theta_hat,Insurance) + log(nrow(Insurance))*3
```


```{r}
BIC
```

## Part B

### (i)

```{r results = FALSE, warning=FALSE}
dlaplace = function(x, mu, sigma){
  exp(-abs(x-mu)/sigma) / (2*sigma)
}


NegLogLikeB = function(theta, data){
  sigma = theta[3]
  n = nrow(data)
  l=0
  for(i in 1:n){
    mu = theta[1]+theta[2]*data$Holders[i]
    l = l + log(dlaplace(data$Claims[i], mu, sigma))
    print(l)
  }
  return(-l)
}


theta_initial=c(0.01,0.1,10)
NegLogLikeB(theta_initial, Insurance)

fit = optim(theta_initial,
            NegLogLikeB,
            data = Insurance)
```


```{r}
ggplot(data=Insurance) +
  geom_line(aes(Holders, fit$par[1]+fit$par[2]*Holders)) +
  geom_point(aes(Holders,Claims))
theta_hat = fit$par
theta_hat

```

### (ii) 

```{r results = FALSE, warning=FALSE}
BIC = 2*NegLogLikeB(theta_hat,Insurance) + log(nrow(Insurance))*3
```


```{r}
BIC
```

## Part C

### (i)

```{r results = FALSE, warning=FALSE}
NegLogLikeC = function(theta,data){
  sigma = theta[3]
  n = nrow(data)
  l=0
  for(i in 1:n){
    if(data$Claims[i] >0){
      mu = theta[1]+theta[2]*data$Holders[i]
      l = l + log(dlnorm(data$Claims[i],meanlog = mu, sdlog = sigma))
      print(l)
    }
  }
  return(-l)
}


theta_initial=c(1, 0, 1)
NegLogLikeC(theta_initial, Insurance)

fit = optim(theta_initial,
            NegLogLikeC,
            data=Insurance)
```


```{r}
ggplot(data=Insurance)+
  geom_line(aes(Holders, fit$par[1]+fit$par[2]*Holders))+
  geom_point(aes(Holders,Claims))
theta_hat = fit$par
theta_hat
```

### (ii)

```{r results = FALSE, warning=FALSE}
BIC=2*NegLogLikeC(theta_hat,Insurance)+log(nrow(Insurance))*3
```


```{r}
BIC
```

## Part D

```{r results = FALSE, warning=FALSE}
NegLogLikeD = function(theta,data){
  scale = theta[3]
  n = nrow(data)
  l=0
  for(i in 1:n){
    shape = theta[1]+theta[2]*data$Holders[i]
    l = l + log(dgamma(data$Claims[i], shape = shape, scale = scale))
    print(l)
  }
  return(-l)
}


theta_initial=c(1, 0, 1)
NegLogLikeD(theta_initial,Insurance)

fit = optim(theta_initial,
            NegLogLikeD,
            data=Insurance)
```


```{r}
ggplot(data=Insurance)+
  geom_line(aes(Holders, fit$par[1]+fit$par[2]*Holders))+
  geom_point(aes(Holders,Claims))
theta_hat = fit$par
theta_hat
```

```{r results = FALSE, warning=FALSE}
BIC=2*NegLogLikeD(theta_hat,Insurance)+log(nrow(Insurance))*3
```


```{r}
BIC
```

## Comparing BIC of all models

Comparing BIC of all the models, we see that BIC of model 2 is least.    
So, we conclude that model 2(Laplace Distribution) is the best fit.

# Problem 5

```{r}
library(tidyverse)
library(quantmod)
getSymbols('TCS.NS')
tail(TCS.NS)

```

```{r}
plot(TCS.NS$TCS.NS.Adjusted)
```

```{r}
getSymbols('^NSEI')
tail(NSEI)
```

```{r}
plot(NSEI$NSEI.Adjusted)
```

```{r}
TCS_rt = diff(log(TCS.NS$TCS.NS.Adjusted))
Nifty_rt = diff(log(NSEI$NSEI.Adjusted))
retrn = cbind.xts(TCS_rt,Nifty_rt)
retrn = na.omit(data.frame(retrn))
plot(retrn$NSEI.Adjusted,retrn$TCS.NS.Adjusted
     ,pch=20
     ,xlab='Market Return'
     ,ylab='TCS Return'
     ,xlim=c(-0.18,0.18)
     ,ylim=c(-0.18,0.18))
grid(col='grey',lty=1)

```

where $\mathbb{E}(\varepsilon)=0$ and $\mathbb{V}ar(\varepsilon)=\sigma^2$.

## Part 1 and 2

```{r}
#For method of moment estimation, the equations are as follows:
#E(TCS)=alpha +beta*E(Nifty)
#E(Nifty(TCS-alpha-beta*Nifty))=0
#E(TCS-alpha-beta*Nifty)^2=sigma^2
attach(retrn)
tail(retrn)

#creating a 2X2 coefficient matrix for solving of system of equations:
x<-array(0,4)
x[1] = 1
x[2] <- mean(NSEI.Adjusted)
x[3] <- mean(NSEI.Adjusted)
x[4] <- mean((NSEI.Adjusted)*(NSEI.Adjusted))
A <- matrix(x,nrow=2)

#inserting another column containing TCS*NSE value named as "C"
retrn <- retrn %>% 
    mutate(C=TCS.NS.Adjusted*NSEI.Adjusted)
attach(retrn)
#storing values in 2X1 matrix for solving of system of equations:
y <- array(0,2)
y[1] <- mean(TCS.NS.Adjusted)
y[2] <- mean(C)
B <- matrix(y,ncol=1)
par_solution <- solve(A,B)

#Now estimating through OLS using lm function
model_ols <- lm(TCS.NS.Adjusted~NSEI.Adjusted,retrn)
summary(model_ols)
matrix_coeff <- summary(model_ols)$coefficients
plot(model_ols)
x<-var(model_ols$residuals)**0.5

#estimating sigma^2
sigma_2 <- mean((TCS.NS.Adjusted-par_solution[1,1]-par_solution[2,1]*NSEI.Adjusted)*(TCS.NS.Adjusted-par_solution[1,1]-par_solution[2,1]*NSEI.Adjusted))

#unbiased estimator of sigma_2
usigma_2 <- sigma_2*(length(TCS.NS.Adjusted)/(length(TCS.NS.Adjusted)-1))
usigma<-usigma_2**0.5

Parameter_List<-data.frame(Parameters=c('alpha','beta','sigma'),
                           MoM=c(par_solution[1,1],par_solution[2,1],usigma_2**0.5),
                           OLS=c(model_ols$coefficients[1],model_ols$coefficients[2],var(model_ols$residuals)**0.5))

```
```{r}
#The data frame consisting of all the estimated parameters
Parameter_List
```

## Part 3

Parameters | Method of Moments         | OLS
-----------|---------------------------|-------------------------------
$\alpha$   |   `r par_solution[1,1]`   |`r model_ols$coefficients[1]`
$\beta$    |   `r par_solution[2,1]`   |`r model_ols$coefficients[2]`
$\sigma$   |   `r usigma`              |`r x`

```{r}
n_ft <-c(log(18200)-log(18000))
n_tcs<-predict(model_ols,newdata = data.frame(NSEI.Adjusted=n_ft))
New_value<-exp(n_tcs+log(3200))

```

## Part 4

The predicted price of TCS is `r New_value`

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

